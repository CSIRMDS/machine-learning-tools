{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = np.array([[1,2,3,4,5],[1,2,3,4,5]])\n",
    "rows,columns = dataset.shape\n",
    "ratio = 1.2\n",
    "number_of_dimensions_to_be_added = int(np.ceil((ratio * np.sqrt(columns))**2))\n",
    "print \"%d irrelevant features are going to be added to the dataset\" %(number_of_dimensions_to_be_added)\n",
    "\n",
    "#randomly choose a feature from the dataset, ie choose number_of_dimensions_to_be_added features\n",
    "\n",
    "#random features to pick\n",
    "features = []\n",
    "for i in range(number_of_dimensions_to_be_added):\n",
    "    features.append(np.random.randint(0,columns))\n",
    "    \n",
    "print features    \n",
    "    \n",
    "#add irreleant features\n",
    "for i,feature in enumerate(features):\n",
    "    appending_features = np.random.choice(dataset[:,feature],rows)\n",
    "    appending_features = appending_features.reshape(1,rows)\n",
    "    dataset= np.insert(dataset,columns+i,appending_features,axis=1)\n",
    "    print dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array([[1,2,3,4,5],[1,3,3,4,5],[1,3,3,4,5],[1,3,3,4,5],[1,3,3,4,5]])\n",
    "\n",
    "#data= np.insert(data,5,[8,2],axis=1)\n",
    "\n",
    "appending_features = np.random.choice(data[:,1],5)\n",
    "appending_features = appending_features.reshape(1,5)\n",
    "print appending_features\n",
    "data= np.insert(data,5,appending_features,axis=1)\n",
    "print data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Euclidean_distance(x,y):\n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "    \n",
    "    \n",
    "def Calculate_distances(distance_matrix,datapoint,seen,dataset):\n",
    "    \n",
    "    rows,columns = distance_matrix.shape\n",
    "    \n",
    "    for i,value in enumerate(range(columns)):\n",
    "        if (seen[0,i]):\n",
    "            distance_matrix[datapoint][i] = distance_matrix[i][datapoint]      \n",
    "        else:\n",
    "            distance_matrix[datapoint][i] = distance(dataset[datapoint,:],dataset[i,:])\n",
    "            \n",
    "def Mahalanobis_distance(heads,point):\n",
    "    \n",
    "    rows,columns = heads.shape\n",
    "    \n",
    "    if (rows>1):\n",
    "        covariance = np.cov(heads,rowvar=0)\n",
    "        var = np.var(heads[:,0],ddof=1)\n",
    "        #covarianceInverse = np.linalg.inv(covariance)\n",
    "        covariancePsuedoInverse = np.linalg.pinv(covariance)\n",
    "    \n",
    "        meanHeads = np.mean(heads,axis=0)\n",
    "        delta = point - meanHeads\n",
    "        \n",
    "        return np.sqrt(np.dot(np.dot(delta, covariancePsuedoInverse), np.transpose(delta)))\n",
    "         \n",
    "    else:\n",
    "        return Euclidean_distance(heads,point)\n",
    "            \n",
    "            \n",
    "def Calculate_heads(dataset,heads,seen,k):\n",
    "    \n",
    "    datasetRows,datasetColumns = dataset.shape\n",
    "    \n",
    "    for i,head in enumerate(arange(1,k)):\n",
    "        \n",
    "        max_distance = -inf\n",
    "        \n",
    "        for j,value in enumerate(seen[0]):\n",
    "            if not value:\n",
    "                point = np.reshape(dataset[j,:],(1,datasetColumns))\n",
    "                distance = Mahalanobis_distance(heads,point)\n",
    "                \n",
    "                if distance > max_distance:\n",
    "                                   max_distance = distance\n",
    "                                   index = j\n",
    "        \n",
    "        seen[0][index] = True\n",
    "        heads = np.insert(heads,i+1,dataset[index,:],axis=0)\n",
    "    \n",
    "    return heads\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[369 949 920 268 916 104 475 652 385 633 882  91 402 975   3  88 633 593\n",
      "  151 633]]\n",
      "[[369 949 920 268 916 104 475 652 385 633 882  91 402 975   3  88 633 593\n",
      "  151 633]\n",
      " [168 259 532 964  22 868 698 138 418 285  62 801 954 405 839 769 152  69\n",
      "  912 525]\n",
      " [835 395 292 379 918  94 319 647 350 900 977  23 140 273 224 478 923 181\n",
      "  227 608]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = np.random.randint(0,1000,(200,20))\n",
    "rows,columns = dataset.shape\n",
    "seen = np.zeros((1,rows), dtype= bool)\n",
    "k = 3\n",
    "\n",
    "#Pick an arbitrary datapoint\n",
    "randomIndex = np.random.randint(0,rows)\n",
    "heads = np.reshape(dataset[randomIndex,:],(1,columns)) \n",
    "seen[0][randomIndex] = True\n",
    "\n",
    "#print dataset\n",
    "print heads.shape\n",
    "#print seen\n",
    "\n",
    "print heads\n",
    "\n",
    "heads =Calculate_heads(dataset,heads,seen,k)\n",
    "\n",
    "print heads\n",
    "\n",
    "#print Mahalanobis_distance(dataset[0:150,:],heads)\n",
    "\n",
    "#print heads \n",
    "\n",
    "#seen = seen.tolist()\n",
    "#print seen[0][1]\n",
    "\n",
    "#heads = np.insert(heads,1,heads,axis=0)\n",
    "#print heads\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.1          5.58888889  -1.77777778   3.64444444  -4.93333333]\n",
      " [  5.58888889  12.01111111  -1.66666667   4.73333333  -6.97777778]\n",
      " [ -1.77777778  -1.66666667   5.33333333   1.           3.33333333]\n",
      " [  3.64444444   4.73333333   1.           7.06666667  -1.86666667]\n",
      " [ -4.93333333  -6.97777778   3.33333333  -1.86666667   7.15555556]]\n",
      "6.1\n",
      "[[  1.00000000e+00   0.00000000e+00   0.00000000e+00   1.11022302e-16\n",
      "   -4.44089210e-16]\n",
      " [  0.00000000e+00   1.00000000e+00   2.22044605e-16   6.66133815e-16\n",
      "    0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00]\n",
      " [  0.00000000e+00   2.22044605e-16   1.11022302e-16   1.00000000e+00\n",
      "    0.00000000e+00]\n",
      " [  4.44089210e-16   0.00000000e+00  -2.22044605e-16   0.00000000e+00\n",
      "    1.00000000e+00]]\n",
      "[[ 2.40843514]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = np.random.randint(0,10,(10,5))\n",
    "cova = np.cov(dataset,rowvar=0)\n",
    "var = np.var(dataset[:,0],ddof=1)\n",
    "print cova\n",
    "print var\n",
    "print cova.dot(np.linalg.inv(cova))\n",
    "\n",
    "x = dataset[1,:]\n",
    "y = dataset[1,:]\n",
    "\n",
    "x = x.reshape(1,5)\n",
    "y = y.reshape(1,5)\n",
    "\n",
    "print Mahalanobis_distance(dataset[0:8,:],y)\n",
    "#print np.mean(dataset,axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_names': array(['setosa', 'versicolor', 'virginica'], \n",
      "      dtype='|S10'), 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n",
      "       [ 4.9,  3. ,  1.4,  0.2],\n",
      "       [ 4.7,  3.2,  1.3,  0.2],\n",
      "       [ 4.6,  3.1,  1.5,  0.2],\n",
      "       [ 5. ,  3.6,  1.4,  0.2],\n",
      "       [ 5.4,  3.9,  1.7,  0.4],\n",
      "       [ 4.6,  3.4,  1.4,  0.3],\n",
      "       [ 5. ,  3.4,  1.5,  0.2],\n",
      "       [ 4.4,  2.9,  1.4,  0.2],\n",
      "       [ 4.9,  3.1,  1.5,  0.1],\n",
      "       [ 5.4,  3.7,  1.5,  0.2],\n",
      "       [ 4.8,  3.4,  1.6,  0.2],\n",
      "       [ 4.8,  3. ,  1.4,  0.1],\n",
      "       [ 4.3,  3. ,  1.1,  0.1],\n",
      "       [ 5.8,  4. ,  1.2,  0.2],\n",
      "       [ 5.7,  4.4,  1.5,  0.4],\n",
      "       [ 5.4,  3.9,  1.3,  0.4],\n",
      "       [ 5.1,  3.5,  1.4,  0.3],\n",
      "       [ 5.7,  3.8,  1.7,  0.3],\n",
      "       [ 5.1,  3.8,  1.5,  0.3],\n",
      "       [ 5.4,  3.4,  1.7,  0.2],\n",
      "       [ 5.1,  3.7,  1.5,  0.4],\n",
      "       [ 4.6,  3.6,  1. ,  0.2],\n",
      "       [ 5.1,  3.3,  1.7,  0.5],\n",
      "       [ 4.8,  3.4,  1.9,  0.2],\n",
      "       [ 5. ,  3. ,  1.6,  0.2],\n",
      "       [ 5. ,  3.4,  1.6,  0.4],\n",
      "       [ 5.2,  3.5,  1.5,  0.2],\n",
      "       [ 5.2,  3.4,  1.4,  0.2],\n",
      "       [ 4.7,  3.2,  1.6,  0.2],\n",
      "       [ 4.8,  3.1,  1.6,  0.2],\n",
      "       [ 5.4,  3.4,  1.5,  0.4],\n",
      "       [ 5.2,  4.1,  1.5,  0.1],\n",
      "       [ 5.5,  4.2,  1.4,  0.2],\n",
      "       [ 4.9,  3.1,  1.5,  0.1],\n",
      "       [ 5. ,  3.2,  1.2,  0.2],\n",
      "       [ 5.5,  3.5,  1.3,  0.2],\n",
      "       [ 4.9,  3.1,  1.5,  0.1],\n",
      "       [ 4.4,  3. ,  1.3,  0.2],\n",
      "       [ 5.1,  3.4,  1.5,  0.2],\n",
      "       [ 5. ,  3.5,  1.3,  0.3],\n",
      "       [ 4.5,  2.3,  1.3,  0.3],\n",
      "       [ 4.4,  3.2,  1.3,  0.2],\n",
      "       [ 5. ,  3.5,  1.6,  0.6],\n",
      "       [ 5.1,  3.8,  1.9,  0.4],\n",
      "       [ 4.8,  3. ,  1.4,  0.3],\n",
      "       [ 5.1,  3.8,  1.6,  0.2],\n",
      "       [ 4.6,  3.2,  1.4,  0.2],\n",
      "       [ 5.3,  3.7,  1.5,  0.2],\n",
      "       [ 5. ,  3.3,  1.4,  0.2],\n",
      "       [ 7. ,  3.2,  4.7,  1.4],\n",
      "       [ 6.4,  3.2,  4.5,  1.5],\n",
      "       [ 6.9,  3.1,  4.9,  1.5],\n",
      "       [ 5.5,  2.3,  4. ,  1.3],\n",
      "       [ 6.5,  2.8,  4.6,  1.5],\n",
      "       [ 5.7,  2.8,  4.5,  1.3],\n",
      "       [ 6.3,  3.3,  4.7,  1.6],\n",
      "       [ 4.9,  2.4,  3.3,  1. ],\n",
      "       [ 6.6,  2.9,  4.6,  1.3],\n",
      "       [ 5.2,  2.7,  3.9,  1.4],\n",
      "       [ 5. ,  2. ,  3.5,  1. ],\n",
      "       [ 5.9,  3. ,  4.2,  1.5],\n",
      "       [ 6. ,  2.2,  4. ,  1. ],\n",
      "       [ 6.1,  2.9,  4.7,  1.4],\n",
      "       [ 5.6,  2.9,  3.6,  1.3],\n",
      "       [ 6.7,  3.1,  4.4,  1.4],\n",
      "       [ 5.6,  3. ,  4.5,  1.5],\n",
      "       [ 5.8,  2.7,  4.1,  1. ],\n",
      "       [ 6.2,  2.2,  4.5,  1.5],\n",
      "       [ 5.6,  2.5,  3.9,  1.1],\n",
      "       [ 5.9,  3.2,  4.8,  1.8],\n",
      "       [ 6.1,  2.8,  4. ,  1.3],\n",
      "       [ 6.3,  2.5,  4.9,  1.5],\n",
      "       [ 6.1,  2.8,  4.7,  1.2],\n",
      "       [ 6.4,  2.9,  4.3,  1.3],\n",
      "       [ 6.6,  3. ,  4.4,  1.4],\n",
      "       [ 6.8,  2.8,  4.8,  1.4],\n",
      "       [ 6.7,  3. ,  5. ,  1.7],\n",
      "       [ 6. ,  2.9,  4.5,  1.5],\n",
      "       [ 5.7,  2.6,  3.5,  1. ],\n",
      "       [ 5.5,  2.4,  3.8,  1.1],\n",
      "       [ 5.5,  2.4,  3.7,  1. ],\n",
      "       [ 5.8,  2.7,  3.9,  1.2],\n",
      "       [ 6. ,  2.7,  5.1,  1.6],\n",
      "       [ 5.4,  3. ,  4.5,  1.5],\n",
      "       [ 6. ,  3.4,  4.5,  1.6],\n",
      "       [ 6.7,  3.1,  4.7,  1.5],\n",
      "       [ 6.3,  2.3,  4.4,  1.3],\n",
      "       [ 5.6,  3. ,  4.1,  1.3],\n",
      "       [ 5.5,  2.5,  4. ,  1.3],\n",
      "       [ 5.5,  2.6,  4.4,  1.2],\n",
      "       [ 6.1,  3. ,  4.6,  1.4],\n",
      "       [ 5.8,  2.6,  4. ,  1.2],\n",
      "       [ 5. ,  2.3,  3.3,  1. ],\n",
      "       [ 5.6,  2.7,  4.2,  1.3],\n",
      "       [ 5.7,  3. ,  4.2,  1.2],\n",
      "       [ 5.7,  2.9,  4.2,  1.3],\n",
      "       [ 6.2,  2.9,  4.3,  1.3],\n",
      "       [ 5.1,  2.5,  3. ,  1.1],\n",
      "       [ 5.7,  2.8,  4.1,  1.3],\n",
      "       [ 6.3,  3.3,  6. ,  2.5],\n",
      "       [ 5.8,  2.7,  5.1,  1.9],\n",
      "       [ 7.1,  3. ,  5.9,  2.1],\n",
      "       [ 6.3,  2.9,  5.6,  1.8],\n",
      "       [ 6.5,  3. ,  5.8,  2.2],\n",
      "       [ 7.6,  3. ,  6.6,  2.1],\n",
      "       [ 4.9,  2.5,  4.5,  1.7],\n",
      "       [ 7.3,  2.9,  6.3,  1.8],\n",
      "       [ 6.7,  2.5,  5.8,  1.8],\n",
      "       [ 7.2,  3.6,  6.1,  2.5],\n",
      "       [ 6.5,  3.2,  5.1,  2. ],\n",
      "       [ 6.4,  2.7,  5.3,  1.9],\n",
      "       [ 6.8,  3. ,  5.5,  2.1],\n",
      "       [ 5.7,  2.5,  5. ,  2. ],\n",
      "       [ 5.8,  2.8,  5.1,  2.4],\n",
      "       [ 6.4,  3.2,  5.3,  2.3],\n",
      "       [ 6.5,  3. ,  5.5,  1.8],\n",
      "       [ 7.7,  3.8,  6.7,  2.2],\n",
      "       [ 7.7,  2.6,  6.9,  2.3],\n",
      "       [ 6. ,  2.2,  5. ,  1.5],\n",
      "       [ 6.9,  3.2,  5.7,  2.3],\n",
      "       [ 5.6,  2.8,  4.9,  2. ],\n",
      "       [ 7.7,  2.8,  6.7,  2. ],\n",
      "       [ 6.3,  2.7,  4.9,  1.8],\n",
      "       [ 6.7,  3.3,  5.7,  2.1],\n",
      "       [ 7.2,  3.2,  6. ,  1.8],\n",
      "       [ 6.2,  2.8,  4.8,  1.8],\n",
      "       [ 6.1,  3. ,  4.9,  1.8],\n",
      "       [ 6.4,  2.8,  5.6,  2.1],\n",
      "       [ 7.2,  3. ,  5.8,  1.6],\n",
      "       [ 7.4,  2.8,  6.1,  1.9],\n",
      "       [ 7.9,  3.8,  6.4,  2. ],\n",
      "       [ 6.4,  2.8,  5.6,  2.2],\n",
      "       [ 6.3,  2.8,  5.1,  1.5],\n",
      "       [ 6.1,  2.6,  5.6,  1.4],\n",
      "       [ 7.7,  3. ,  6.1,  2.3],\n",
      "       [ 6.3,  3.4,  5.6,  2.4],\n",
      "       [ 6.4,  3.1,  5.5,  1.8],\n",
      "       [ 6. ,  3. ,  4.8,  1.8],\n",
      "       [ 6.9,  3.1,  5.4,  2.1],\n",
      "       [ 6.7,  3.1,  5.6,  2.4],\n",
      "       [ 6.9,  3.1,  5.1,  2.3],\n",
      "       [ 5.8,  2.7,  5.1,  1.9],\n",
      "       [ 6.8,  3.2,  5.9,  2.3],\n",
      "       [ 6.7,  3.3,  5.7,  2.5],\n",
      "       [ 6.7,  3. ,  5.2,  2.3],\n",
      "       [ 6.3,  2.5,  5. ,  1.9],\n",
      "       [ 6.5,  3. ,  5.2,  2. ],\n",
      "       [ 6.2,  3.4,  5.4,  2.3],\n",
      "       [ 5.9,  3. ,  5.1,  1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'DESCR': 'Iris Plants Database\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "print load_iris()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
