{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info\n",
    "\n",
    "This notebook introduces the Auto Enconder Neural Network using Theano. Simply we take in the data, compress it and then decompress it. \n",
    "\n",
    "### TODO\n",
    "\n",
    "* Add an error function\n",
    "* Compare different compressions\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "* mdiale@csir.co.za"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For further details on implementation of the code visit:\n",
    "\n",
    "#https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano as th\n",
    "\n",
    "from numpy import random as rng\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from theano import tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class AutoEncoder(object):\n",
    "    def __init__(self, X, hidden_size, activation_function,\n",
    "                 output_function):\n",
    "        #X is the data, an m x n numpy matrix\n",
    "        #where rows correspond to datapoints\n",
    "        #and columns correspond to features.\n",
    "        assert type(X) is np.ndarray\n",
    "        assert len(X.shape)==2\n",
    "        self.X=X\n",
    "        self.X=th.shared(name='X', value=np.asarray(self.X, \n",
    "                         dtype=th.config.floatX),borrow=True)\n",
    "        #The config.floatX and borrow=True stuff is to get this to run\n",
    "        #fast on the gpu. I recommend just doing this without thinking about\n",
    "        #it until you understand the code as a whole, then learning more\n",
    "        #about gpus and theano.\n",
    "        self.n = X.shape[1]\n",
    "        self.m = X.shape[0]\n",
    "        #Hidden_size is the number of neurons in the hidden layer, an int.\n",
    "        assert type(hidden_size) is int\n",
    "        assert hidden_size > 0\n",
    "        self.hidden_size=hidden_size\n",
    "        initial_W = np.asarray(rng.uniform(\n",
    "                 low=-4 * np.sqrt(6. / (self.hidden_size + self.n)),\n",
    "                 high=4 * np.sqrt(6. / (self.hidden_size + self.n)),\n",
    "                 size=(self.n, self.hidden_size)), dtype=th.config.floatX)\n",
    "        self.W = th.shared(value=initial_W, name='W', borrow=True)\n",
    "        self.b1 = th.shared(name='b1', value=np.zeros(shape=(self.hidden_size,),\n",
    "                            dtype=th.config.floatX),borrow=True)\n",
    "        self.b2 = th.shared(name='b2', value=np.zeros(shape=(self.n,),\n",
    "                            dtype=th.config.floatX),borrow=True)\n",
    "        self.activation_function=activation_function\n",
    "        self.output_function=output_function\n",
    "                     \n",
    "    def train(self, n_epochs=10000, mini_batch_size=1, learning_rate=0.1):\n",
    "        index = T.lscalar()\n",
    "        x=T.matrix('x')\n",
    "        params = [self.W, self.b1, self.b2]\n",
    "        hidden = self.activation_function(T.dot(x, self.W)+self.b1)\n",
    "        output = T.dot(hidden,T.transpose(self.W))+self.b2\n",
    "        output = self.output_function(output)\n",
    "         \n",
    "        #Use cross-entropy loss.\n",
    "        L = -T.sum(x*T.log(output) + (1-x)*T.log(1-output), axis=1)\n",
    "        cost=L.mean()       \n",
    "        updates=[]\n",
    "         \n",
    "        #Return gradient with respect to W, b1, b2.\n",
    "        gparams = T.grad(cost,params)\n",
    "         \n",
    "        #Create a list of 2 tuples for updates.\n",
    "        for param, gparam in zip(params, gparams):\n",
    "            updates.append((param, param-learning_rate*gparam))\n",
    "         \n",
    "        #Train given a mini-batch of the data.\n",
    "        train = th.function(inputs=[index], outputs=[cost], updates=updates,\n",
    "                            givens={x:self.X[index:index+mini_batch_size,:]})\n",
    "                             \n",
    " \n",
    "        import time\n",
    "        start_time = time.clock()\n",
    "        for epoch in xrange(n_epochs):\n",
    "            #print \"Epoch:\",epoch\n",
    "            for row in xrange(0,self.m, mini_batch_size):\n",
    "                train(row)\n",
    "        end_time = time.clock()\n",
    "        print \"Average time per epoch=\", (end_time-start_time)/n_epochs\n",
    "        \n",
    "     \n",
    "    def get_weights(self):\n",
    "        return [self.W.get_value(), self.b1.get_value(), self.b2.get_value()]         \n",
    "               \n",
    "    def get_hidden(self,data):\n",
    "        x=T.dmatrix('x')\n",
    "        hidden = self.activation_function(T.dot(x,self.W)+self.b1)\n",
    "        transformed_data = th.function(inputs=[x], outputs=hidden)\n",
    "        return transformed_data(data)  \n",
    "    \n",
    "    def get_output(self,data):\n",
    "        y = T.dmatrix('y')\n",
    "        output = self.output_function(T.dot(y,T.transpose(self.W))+self.b2)\n",
    "        reconstructed_data = th.function(inputs=[y], outputs=output)\n",
    "        return reconstructed_data(data)\n",
    "    \n",
    "    def get_reconstructionError(self,X,Z):\n",
    "        # cross-entropy of the reconstruction\n",
    "        x=T.dmatrix('x')\n",
    "        z=T.dmatrix('z')\n",
    "        L = -T.sum(x*T.log(z) + (1-x)*T.log(1-z), axis=1)\n",
    "        cost=L.mean()\n",
    "        error = th.function(inputs=[x,z], outputs=cost)\n",
    "        return error(X,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def m_test(data):\n",
    "    X=data\n",
    "    activation_function = T.nnet.sigmoid\n",
    "    output_function=activation_function\n",
    "    hidden_features = 9\n",
    "    A = AutoEncoder(X, hidden_features, activation_function, output_function)\n",
    "    A.train()\n",
    "    W=np.transpose(A.get_weights()[0])\n",
    "    ####################################################\n",
    "    Y = A.get_hidden(X)\n",
    "    Z = A.get_output(Y)\n",
    "    ####################################################\n",
    "    print X\n",
    "    print \"##############################bellow is a compressed data##################################\"\n",
    "    print Y\n",
    "    print \"##############################bellow is a reconstructed data##################################\"\n",
    "    print Z\n",
    "    print \"Print reconstruction error is : \"+ str(A.get_reconstructionError(X,Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177, 14)\n"
     ]
    }
   ],
   "source": [
    "#data = np.array([[1,2,5,3,4,5,6,9,8,7,4,5,6,9,8,7],[2,3,1,6,9,8,3,2,6,5,4,5,6,9,8,7],[1,2,5,8,9,6,4,7,5,3,4,5,6,9,8,7],[1,2,5,3,9,6,4,7,5,3,4,5,6,9,8,7],[1,2,5,1,9,6,4,7,5,3,1,2,5,3,9,6],[1,2,5,8,9,6,4,7,5,3,4,5,6,9,8,7],[1,2,9,8,9,6,4,7,5,3,6,4,7,5,3,4],[1,2,7,6,9,6,6,4,7,5,3,4,4,7,5,3],[1,2,5,8,9,6,4,7,5,3,4,5,6,9,8,7],[1,4,5,6,9,8,7,2,3,8,9,6,4,7,5,3]],dtype='f4')\n",
    "data = pd.read_csv(open('../../data/wine/wine.data'))\n",
    "columns = ['class','alcohol','malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', \n",
    "           'nonflavanoid phenols', 'proanthocyanins',  'color_intensity', 'hue', 'od_of_diluted_wines', 'proline']\n",
    "data.columns = columns\n",
    "class_coloumns = columns[1:]\n",
    "X = data[class_coloumns].values\n",
    "Y =  data[columns[0]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.33, random_state=42)\n",
    "X_train = np.array(X_train)\n",
    "print data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Scaling features to a range\n",
    "X_train = np.array(X)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train = X_train_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per epoch= 0.0182473663\n",
      "[[ 0.57105263  0.2055336   0.4171123  ...,  0.46341463  0.78021978\n",
      "   0.55064194]\n",
      " [ 0.56052632  0.3201581   0.70053476 ...,  0.44715447  0.6959707\n",
      "   0.64693295]\n",
      " [ 0.87894737  0.23913043  0.60962567 ...,  0.30894309  0.7985348\n",
      "   0.85734665]\n",
      " ..., \n",
      " [ 0.58947368  0.69960474  0.48128342 ...,  0.08943089  0.10622711\n",
      "   0.39728959]\n",
      " [ 0.56315789  0.36561265  0.54010695 ...,  0.09756098  0.12820513\n",
      "   0.40085592]\n",
      " [ 0.81578947  0.66403162  0.73796791 ...,  0.10569106  0.12087912\n",
      "   0.20114123]]\n",
      "##############################bellow is a compressed data##################################\n",
      "[[ 0.53191579  0.67945679  0.14413347 ...,  0.85945389  0.57492772\n",
      "   0.28071271]\n",
      " [ 0.76723615  0.90417271  0.37143967 ...,  0.78249603  0.21496134\n",
      "   0.34333748]\n",
      " [ 0.8688881   0.90659504  0.09918177 ...,  0.67238429  0.20567826\n",
      "   0.1172632 ]\n",
      " ..., \n",
      " [ 0.19220446  0.34121969  0.07090652 ...,  0.1567933   0.38160291\n",
      "   0.38684999]\n",
      " [ 0.39872     0.30047859  0.12041491 ...,  0.14479197  0.32267703\n",
      "   0.36137145]\n",
      " [ 0.47482032  0.11480286  0.17717529 ...,  0.09728573  0.25048052\n",
      "   0.67447141]]\n",
      "##############################bellow is a reconstructed data##################################\n",
      "[[ 0.58906905  0.19857413  0.31474949 ...,  0.49385945  0.69320797\n",
      "   0.627792  ]\n",
      " [ 0.61194231  0.27296072  0.59224399 ...,  0.46835056  0.73150482\n",
      "   0.69440123]\n",
      " [ 0.80995579  0.23554747  0.62288591 ...,  0.4599547   0.74307627\n",
      "   0.81227977]\n",
      " ..., \n",
      " [ 0.58731475  0.70786743  0.54978498 ...,  0.10270721  0.08872144\n",
      "   0.35479744]\n",
      " [ 0.55183033  0.37278852  0.56170908 ...,  0.16898598  0.07091281\n",
      "   0.41493226]\n",
      " [ 0.80733394  0.71345551  0.71965163 ...,  0.12764568  0.17613741\n",
      "   0.21678152]]\n",
      "Print reconstruction error is : 7.50929574687\n"
     ]
    }
   ],
   "source": [
    "m_test(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
